{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#바뀜 확인\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "#import missingno as msno\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './001.data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./001.data/train.csv' does not exist: b'./001.data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0ef6725d73fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspecs_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'specs.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_labels_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'train_labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msub_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./001.data/train.csv' does not exist: b'./001.data/train.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(path + 'train.csv')\n",
    "test_df = pd.read_csv(path + 'test.csv')\n",
    "specs_df = pd.read_csv(path + 'specs.csv')\n",
    "train_labels_df = pd.read_csv(path +'train_labels.csv')\n",
    "sub_df = pd.read_csv(path +'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape, specs_df.shape,train_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T03:02:36.030370Z",
     "start_time": "2019-10-19T03:02:36.019369Z"
    }
   },
   "outputs": [],
   "source": [
    "def info_df(data):\n",
    "    '''\n",
    "    data의 type, null_count, null_rate를 알려주는 함수 \n",
    "    추가로 unique한 데이터의 수를 알려줌 (속도가 느리므로 필요시 주석처리 필요)\n",
    "    '''\n",
    "    info_df = pd.DataFrame({\"type\":data.dtypes,\n",
    "                            'null_count':data.isnull().sum(),\n",
    "                           'null_rate':data.isnull().sum()/data.isnull().count() * 100})\n",
    "    \n",
    "#     cols = data.columns.values\n",
    "    \n",
    "#     uni_count =[]\n",
    "#     for col in cols:\n",
    "#         uni_count.append(len(data[col].unique()))\n",
    "#     info_df['uni_count'] = uni_count\n",
    "    \n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "def model_fi(model):\n",
    "    fi_df = pd.DataFrame({'importance': model.feature_importances_},index = train_X.columns)\n",
    "    fi_df = fi_df.sort_values(by ='importance', ascending=False)\n",
    "    fi_df['importance'] = round(fi_df['importance'],4)\n",
    "    \n",
    "    return fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GS_event_G(data):\n",
    "    '''\n",
    "    train_df/test_df를 넣으면 game_session별로 묶어진 event_code별 카운트를 반환\n",
    "    NaN은 0으로 체움\n",
    "    '''\n",
    "    data['values'] = 1\n",
    "    GS_event = data.pivot_table(index = 'game_session', columns = 'event_code', values ='values',aggfunc= 'sum')\n",
    "    GS_event.fillna(0,inplace= True)\n",
    "    GS_event = GS_event.reset_index()\n",
    "    \n",
    "    return GS_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "GS_event = GS_event_G(train_df)\n",
    "t_GS_event = GS_event_G(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_event.shape, t_GS_event.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def GS_max_G(data,data2):\n",
    "    '''\n",
    "    data에 train_df/test_df를 넣으면 game_session별로 묶어진 timestamp,event_count,game_time,type의 max를 반환\n",
    "    data2에 GS_event/t_GS_event를 넣으면 merge \n",
    "    다소오래걸림\n",
    "    '''\n",
    "    #max값 산출\n",
    "    GS_max= data.groupby('game_session')[\"timestamp\",\"installation_id\",\"event_count\",\"game_time\",\"type\"].max()\n",
    "    GS_max = GS_max.reset_index()\n",
    "    GS_max = GS_max.rename(columns={\"timestamp\":\"timestamp_max\"})\n",
    "    \n",
    "    #min값 산출\n",
    "    GS_min = data.groupby('game_session')[\"timestamp\"].min()\n",
    "    GS_min = GS_min.reset_index()\n",
    "    GS_min = GS_min.rename(columns={\"timestamp\":\"timestamp_min\"})\n",
    "    \n",
    "    #min,max 결합\n",
    "    GS_m= pd.merge(GS_max,GS_min)\n",
    "    GS_m = GS_m.sort_values(['installation_id','timestamp_max'])\n",
    "    \n",
    "    #GS_m과,GS_event의 결합\n",
    "    GS_me = pd.merge(GS_m,data2)\n",
    "    \n",
    "    return GS_me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GS_me = GS_max_G(train_df,GS_event)\n",
    "t_GS_me = GS_max_G(test_df,t_GS_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_me.shape, t_GS_me.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GS_title_G(data,col1,col2):\n",
    "    '''\n",
    "    data를 넣으면 game_session이 속한 title/world을 1로 표시하여 merge\n",
    "    result는 world를 받고\n",
    "    result2는 title을 받는걸 권장\n",
    "    '''\n",
    "    data['values'] = 1\n",
    "    result = pd.DataFrame()\n",
    "    result2 = pd.DataFrame()\n",
    "    \n",
    "    #world\n",
    "    result = data.pivot_table(index = 'game_session',\n",
    "                                      columns= col1,\n",
    "                                      values = 'values',\n",
    "                                      aggfunc= 'count')\n",
    "    for col in result.columns.values:\n",
    "        result[col] = result[col].apply(lambda x : 1 if x>0 else 0)\n",
    "    \n",
    "    #title\n",
    "    result2 = data.pivot_table(index = 'game_session',\n",
    "                                  columns= col2,\n",
    "                                  values = 'values',\n",
    "                                  aggfunc= 'count')\n",
    "    for col in result2.columns.values:\n",
    "        result2[col] = result2[col].apply(lambda x : 1 if x>0 else 0)\n",
    "    \n",
    "    final = pd.merge(result,result2, right_index= True, left_index = True)\n",
    "    final = final.reset_index()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "GS_title = GS_title_G(train_df, \"world\",\"title\")\n",
    "t_GS_title = GS_title_G(test_df, \"world\",\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_title.shape, t_GS_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_met= pd.merge(GS_me,GS_title)\n",
    "t_GS_met= pd.merge(t_GS_me,t_GS_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_met.shape, t_GS_met.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GS_labels = pd.merge(GS_met,train_labels_df[[\"game_session\",\"accuracy_group\"]],how='left',on= 'game_session')\n",
    "GS_labels = GS_labels.sort_values(['installation_id','timestamp_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(data,col1,col2):\n",
    "    data[col1] = pd.to_datetime(data[col1])\n",
    "    data[col2] = pd.to_datetime(data[col2])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GS_time = to_datetime(GS_labels,\"timestamp_max\",\"timestamp_min\")\n",
    "t_GS_time = to_datetime(t_GS_met,\"timestamp_max\",\"timestamp_min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_time.shape,t_GS_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wide(data,data2, ):\n",
    "    '''\n",
    "    GS_labels와 GS_event를 merge한 data을 넣으면 wide 포맷으로 변환\n",
    "    1차 for문은 unique한 installment_id를 반복하기 위해\n",
    "        2차 for문은 각 assessemnt의 위치를 반환하기 위함\n",
    "    최종적으로 생략했던 train_label_df와 병합\n",
    "       \n",
    "    '''\n",
    "    #기본변수선언\n",
    "    # 고유 installation_id \n",
    "    uni_iids = data.installation_id.unique()\n",
    "\n",
    "    #game_session(assessment)별 결과를 임시로 담아놓을 장소\n",
    "    world_ls = list(t_GS_title.iloc[:,1:].columns.values)\n",
    "    event_ls = list(t_GS_event.iloc[:,1:].columns.values)\n",
    "    base_ls = ['installation_id','game_session','event_count',\n",
    "                  'Activity_c','Assessment_c' ,'Clip_c', 'Game_c',\n",
    "                  'Activity_t','Assessment_t' ,'Clip_t', 'Game_t',\n",
    "                  'timestamp_max','timestamp_min','time_gap','game_time','ass_gt']\n",
    "    result_col = base_ls + event_ls + world_ls\n",
    "    \n",
    "    pure_result = pd.DataFrame(columns= result_col, \n",
    "                               data= [[0] * len(result_col)])\n",
    "    pure_result['time_gap'] = datetime.timedelta(0)\n",
    "    \n",
    "    #type별 count_time을 보관하기 위한 장소 \n",
    "    type_df = pd.DataFrame(columns = ['Activity','Assessment' ,'Clip', 'Game'],\n",
    "                           data = [[0,0,0,0]])\n",
    "    #event_code별, game_time 등 sum값을 보관하기 위한 장소\n",
    "    sum_list = data.columns.drop(['installation_id','type','game_session',\n",
    "                                  'timestamp_max','timestamp_min','accuracy_group']).values\n",
    "    \n",
    "    #최종결과가 담길 장소\n",
    "    result_df =pd.DataFrame()\n",
    "    \n",
    "    #Assessment의 범위 설정\n",
    "    n = 0\n",
    "    m = 0\n",
    "\n",
    "    for uni_iid in uni_iids:\n",
    "        as_indexs = data.loc[(data.installation_id == uni_iid)&\n",
    "                              (data.type == 'Assessment')&\n",
    "                              (data.accuracy_group.notnull())].index.values\n",
    "        last_index = data[data.installation_id == uni_iid].index.max()\n",
    "\n",
    "        for as_index in as_indexs:\n",
    "            result = pure_result\n",
    "            m = as_index\n",
    "            try: \n",
    "                #type count\n",
    "                type_dict = data.iloc[n:m].groupby('type')['installation_id'].count().to_dict()\n",
    "                temp = (type_df + pd.DataFrame([type_dict])).fillna(0)\n",
    "                temp.columns = ['Activity_c','Assessment_c' ,'Clip_c', 'Game_c']\n",
    "                result[['Activity_c','Assessment_c' ,'Clip_c', 'Game_c']] = temp\n",
    "\n",
    "                #type time\n",
    "                type_dict = data.iloc[n:m].groupby('type')['game_time'].sum().to_dict()\n",
    "                temp = (type_df + pd.DataFrame([type_dict])).fillna(0)\n",
    "                temp.columns = ['Activity_t','Assessment_t' ,'Clip_t', 'Game_t']\n",
    "                result[['Activity_t','Assessment_t' ,'Clip_t', 'Game_t']] = temp\n",
    "\n",
    "                #time_gap   \n",
    "                result['timestamp_max'] = data.iloc[m].timestamp_max\n",
    "                result['timestamp_min'] = data.iloc[n].timestamp_min\n",
    "                time_gap = data.iloc[m].timestamp_max - data.iloc[n].timestamp_min\n",
    "                result['time_gap'] =time_gap \n",
    "\n",
    "#                 #ass_gametime \n",
    "#                 result['ass_gt'] = data.loc[m].game_time\n",
    "\n",
    "                #event_code별, game_time 등의 sum \n",
    "                sum_df = data[n:m].groupby('installation_id')[sum_list].sum().reset_index().fillna(0)\n",
    "                sum_df = sum_df.drop('installation_id',axis= 1)\n",
    "                result[sum_df.columns.values] = sum_df \n",
    "\n",
    "                #id\n",
    "                result['installation_id'] = uni_iid\n",
    "                result['game_session'] = data.iloc[m].game_session\n",
    "\n",
    "                #result\n",
    "                result_df= result_df.append(result)\n",
    "            except Exception as e:\n",
    "                #처음이 assessment인 경우 에러발생으로 인해 기본갑 0 부여 \n",
    "                print(e)\n",
    "                result['installation_id'] = uni_iid\n",
    "                result['game_session'] = data.iloc[m].game_session\n",
    "\n",
    "                result_df= result_df.append(result)\n",
    "                \n",
    "        #각 installment_id의 가장 마지막 row에 +1\n",
    "        n= last_index+1\n",
    "    result_df.fillna(0,inplace=True)   \n",
    "        \n",
    "    #최종적으로 생략되었던 train_labels 부분 추가(accuracy_group 등)\n",
    "    wide = pd.merge(result_df,data2.drop(\"installation_id\",axis= 1))\n",
    "    \n",
    "    return wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_wide(data):\n",
    "    '''\n",
    "    train_df와 달리 마지막 assessement까지의 데이터를 결합\n",
    "       \n",
    "    '''\n",
    "    #기본변수선언\n",
    "    # 고유 installation_id \n",
    "    uni_iids = data.installation_id.unique()\n",
    "\n",
    "    #game_session(assessment)별 결과를 임시로 담아놓을 장소\n",
    "    world_ls = list(t_GS_title.iloc[:,1:].columns.values)\n",
    "    event_ls = list(t_GS_event.iloc[:,1:].columns.values)\n",
    "    base_ls = ['installation_id','game_session','event_count',\n",
    "                  'Activity_c','Assessment_c' ,'Clip_c', 'Game_c',\n",
    "                  'Activity_t','Assessment_t' ,'Clip_t', 'Game_t',\n",
    "                  'timestamp_max','timestamp_min','time_gap','game_time','ass_gt']\n",
    "    result_col = base_ls + event_ls + world_ls\n",
    "\n",
    "    \n",
    "    pure_result = pd.DataFrame(columns= result_col,\n",
    "                               data= [[0] * len(result_col)])\n",
    "    pure_result['time_gap'] =datetime.timedelta(0)\n",
    "    \n",
    "    #type별 count_time을 보관하기 위한 장소 \n",
    "    type_df = pd.DataFrame(columns = ['Activity','Assessment' ,'Clip', 'Game'],\n",
    "                           data = [[0,0,0,0]])\n",
    "    #event_code별, game_time 등 sum값을 보관하기 위한 장소\n",
    "    sum_list = data.columns.drop(['installation_id','type','game_session',\n",
    "                                  'timestamp_max','timestamp_min']).values\n",
    "    \n",
    "    #최종결과가 담길 장소\n",
    "    result_df =pd.DataFrame()\n",
    "    \n",
    "    #Assessment의 범위 설정\n",
    "    n = 0\n",
    "    m = 0\n",
    "\n",
    "    for uni_iid in uni_iids:\n",
    "        as_indexs = data.loc[(data.installation_id == uni_iid)&\n",
    "                              (data.type == 'Assessment')].index.values\n",
    "        last_index = data[data.installation_id == uni_iid].index.max()\n",
    "\n",
    "        result = pure_result\n",
    "        m = as_indexs[-1]\n",
    "        try: \n",
    "            #type count\n",
    "            type_dict = data.iloc[n:m].groupby('type')['installation_id'].count().to_dict()\n",
    "            temp = (type_df + pd.DataFrame([type_dict])).fillna(0)\n",
    "            temp.columns = ['Activity_c','Assessment_c' ,'Clip_c', 'Game_c']\n",
    "            result[['Activity_c','Assessment_c' ,'Clip_c', 'Game_c']] = temp\n",
    "\n",
    "            #type time\n",
    "            type_dict = data.iloc[n:m].groupby('type')['game_time'].sum().to_dict()\n",
    "            temp = (type_df + pd.DataFrame([type_dict])).fillna(0)\n",
    "            temp.columns = ['Activity_t','Assessment_t' ,'Clip_t', 'Game_t']\n",
    "            result[['Activity_t','Assessment_t' ,'Clip_t', 'Game_t']] = temp\n",
    "\n",
    "            #time_gap   \n",
    "            result['timestamp_max'] = data.iloc[m].timestamp_max\n",
    "            result['timestamp_min'] = data.iloc[n].timestamp_min\n",
    "            time_gap = data.iloc[m].timestamp_max - data.iloc[n].timestamp_min\n",
    "            result['time_gap'] =time_gap \n",
    "\n",
    "            #ass_gametime \n",
    "            result['ass_gt'] = data.loc[m].game_time\n",
    "\n",
    "            #event_code별, game_time 등의 sum \n",
    "            sum_df = data[n:m].groupby('installation_id')[sum_list].sum().reset_index()\n",
    "            sum_df = sum_df.drop('installation_id',axis= 1)\n",
    "            result[sum_df.columns.values] = sum_df \n",
    "\n",
    "            #id\n",
    "            result['installation_id'] = uni_iid\n",
    "            result['game_session'] = data.iloc[m].game_session\n",
    "\n",
    "            #result\n",
    "            result_df= result_df.append(result)\n",
    "        except Exception as e:\n",
    "            #처음이 assessment인 경우 에러발생으로 인해 기본갑 0 부여 \n",
    "            print(e)\n",
    "            result['installation_id'] = uni_iid\n",
    "            result['game_session'] = data.iloc[m].game_session\n",
    "\n",
    "            result_df= result_df.append(result)\n",
    "                \n",
    "        #각 installment_id의 가장 마지막 row에 +1\n",
    "        n= last_index+1\n",
    "    result_df.fillna(0,inplace=True)   \n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wide = make_wide(GS_time,train_labels_df)\n",
    "t_wide = test_make_wide(t_GS_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide.to_csv('wide.csv',index=False)\n",
    "t_wide.to_csv('t_wide.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide.shape, t_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test set title추가\n",
    "t_wide = pd.merge(t_wide,test_df[['game_session','title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_col(data,col):\n",
    "    '''\n",
    "    one_hot인코딩해줌\n",
    "    '''\n",
    "    dump = pd.get_dummies(data[col])\n",
    "    dump.columns=['Bird Measurer (Assessment)_f','Cart Balancer (Assessment)_f',\n",
    "                  'Cauldron Filler (Assessment)_f','Chest Sorter (Assessment)_f',\n",
    "                  'Mushroom Sorter (Assessment)_f']\n",
    "    data=pd.concat([data, dump], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_ohe = one_hot_col(wide,'title')\n",
    "t_wide_ohe = one_hot_col(t_wide,'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_s(data,col):\n",
    "    data[col+\"_s\"] = data[col].apply(lambda x: x.seconds)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wide_s = date_to_s(wide_ohe,'time_gap')\n",
    "t_wide_s = date_to_s(t_wide_ohe,'time_gap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_drop = ['installation_id','game_session','event_count','Clip_t','time_gap',\n",
    "             'timestamp_max', 'timestamp_min','title','accuracy','num_correct', 'num_incorrect','ass_gt']\n",
    "list_drop2 = ['installation_id','game_session','event_count','Clip_t','time_gap',\n",
    "             'timestamp_max', 'timestamp_min','title','ass_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_fin=wide_s.drop(list_drop,axis=1)\n",
    "t_wide_fin=t_wide_s.drop(list_drop2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_fin.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_wide_fin.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wide_fin.shape,t_wide_fin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상관관계분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_sameple = wide_fin\n",
    "heat_sameple = pd.concat([wide_fin.accuracy_group,heat_sameple],axis=1)\n",
    "plt.figure(figsize= (10,10))\n",
    "sns.heatmap(heat_sameple.corr(),vmax = 1, vmin= -1,cmap=\"RdYlBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_sameple = wide_fin.iloc[:,20:40]\n",
    "heat_sameple = pd.concat([wide_fin.accuracy_group,heat_sameple],axis=1)\n",
    "plt.figure(figsize= (10,10))\n",
    "sns.heatmap(heat_sameple.corr(),vmax = 1, vmin= -1,cmap=\"RdYlBu\",annot= True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차 모델 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wide_fin.accuracy_group\n",
    "X = wide_fin.drop('accuracy_group',axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = lgbm.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.score(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model_1.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from  sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_df(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-1] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(test_y,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fi(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wide_fin.accuracy_group\n",
    "X_train = wide_fin.drop('accuracy_group',axis= 1)\n",
    "X_test = t_wide_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = lgbm.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.score(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fi(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1차제출파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['installation_id'] = t_wide_s.installation_id\n",
    "pred_df['accuracy_group'] = pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_d = sub_df.drop('accuracy_group',axis =1)\n",
    "submission = pd.merge(sub_d,pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}